{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from IPython.display import display\n",
    "from ipywidgets import HBox, VBox\n",
    "from collections import OrderedDict\n",
    "from fuzzywuzzy import process \n",
    "from fuzzywuzzy import fuzz\n",
    "pd.options.display.html.table_schema = True\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.max_colwidth = 60\n",
    "pd.options.display.width = 180\n",
    "pd.options.display.expand_frame_repr = True\n",
    "#import google.cloud.automl_v1beta1 as automl_v1beta1\n",
    "from google.cloud import automl_v1beta1\n",
    "from google.cloud.automl_v1beta1.proto import service_pb2\n",
    "import textlib\n",
    "from textlib import load_raw\n",
    "from textlib import normalize_sent\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#The-H&amp;K-Lawyer-Notebook\" data-toc-modified-id=\"The-H&amp;K-Lawyer-Notebook-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>The H&amp;K Lawyer Notebook</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification-Tool\" data-toc-modified-id=\"Classification-Tool-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Classification Tool</a></span></li><li><span><a href=\"#Bulk-Compare\" data-toc-modified-id=\"Bulk-Compare-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Bulk Compare</a></span></li><li><span><a href=\"#Find-Precedent\" data-toc-modified-id=\"Find-Precedent-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Find Precedent</a></span></li><li><span><a href=\"#Clustering-and-Text-Similarity-Models\" data-toc-modified-id=\"Clustering-and-Text-Similarity-Models-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Clustering and Text Similarity Models</a></span></li><li><span><a href=\"#QuickCAN\" data-toc-modified-id=\"QuickCAN-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>QuickCAN</a></span></li><li><span><a href=\"#QuickCAR\" data-toc-modified-id=\"QuickCAR-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>QuickCAR</a></span></li><li><span><a href=\"#Signature-Page-Generator\" data-toc-modified-id=\"Signature-Page-Generator-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Signature Page Generator</a></span></li><li><span><a href=\"#Closing-Binder-Generator\" data-toc-modified-id=\"Closing-Binder-Generator-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Closing Binder Generator</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup as BeautifulSoup\n",
    "from openpyxl import Workbook as Workbook\n",
    "\n",
    "def save_list_as_excel(path, clean_sent_list, raw_sent_list):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    column_cell_A = 'A'\n",
    "    column_cell_B = 'B'\n",
    "    column_cell_C = 'C'\n",
    "    ws[column_cell_A + str(1)] = 'text'\n",
    "    ws[column_cell_B + str(1)] = 'label'\n",
    "    ws[column_cell_C + str(1)] = 'raw'\n",
    "    list_len = len(clean_sent_list)\n",
    "    for i in range(0, list_len):\n",
    "        ws[column_cell_A + str(i + 2)] = clean_sent_list[i]\n",
    "        ws[column_cell_B + str(i + 2)] = 'label'\n",
    "        ws[column_cell_C + str(i + 2)] = raw_sent_list[i]\n",
    "    wb.save(path)\n",
    "    print('Excel write complete')\n",
    "\n",
    "\n",
    "def html_from_file_no_tags(file_path):\n",
    "    with open(file_path, 'rb') as myfile:\n",
    "        raw_text = myfile.read()\n",
    "    bsObj = BeautifulSoup(raw_text, 'lxml').text\n",
    "    return bsObj\n",
    "\n",
    "\n",
    "def normalize_sent(sent):\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    sent = re.sub(r'[^a-zA-Z\\s]', '', sent, re.I | re.A)\n",
    "    sent = sent.strip()\n",
    "    tokens = wpt.tokenize(sent)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    sent = ' '.join(filtered_tokens).lower()\n",
    "    return sent\n",
    "\n",
    "\n",
    "def html_from_file_no_tags(file_path):\n",
    "    with open(file_path, 'rb') as myfile:\n",
    "        raw_text = myfile.read()\n",
    "    bsObj = BeautifulSoup(raw_text, 'lxml').text\n",
    "    return bsObj\n",
    "\n",
    "\n",
    "def load_normal_with_stopwords(path):\n",
    "    doc = str(load_raw(path))\n",
    "    return normalize_document_return_list(doc)\n",
    "\n",
    "\n",
    "def load_normal_no_stopwords(path):\n",
    "    doc = load_raw(path)\n",
    "    norm = normalize_document_return_list(doc)\n",
    "    clean_sent = []\n",
    "    for sent in norm:\n",
    "        clean = remove_stop_words(sent)\n",
    "        clean_sent.append(clean)\n",
    "    return clean_sent\n",
    "\n",
    "\n",
    "def list_from_directory(path):\n",
    "    list_of_text = []\n",
    "    for file in os.listdir(path):\n",
    "        filename = os.fsdecode(file)\n",
    "        file_path = path + filename\n",
    "        text = load_raw(file_path)\n",
    "        text = str(text)\n",
    "        list_of_text.append(text)\n",
    "\n",
    "\n",
    "def load_excel(path):\n",
    "    print('finish')\n",
    "\n",
    "\n",
    "def load_list_from_csv(path):\n",
    "    with open(path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        list_raw = list(reader)\n",
    "        list_clean = []\n",
    "        for i in list_raw:\n",
    "            x = ''.join(i)\n",
    "            list_clean.append(x)\n",
    "        list_clean_two = []\n",
    "        for i in list_clean:\n",
    "            x = ''.join(i)\n",
    "            list_clean_two.append(x)\n",
    "        return list_clean_two\n",
    "\n",
    "\n",
    "def load_raw(path, tags=False):\n",
    "    if path.endswith('html'):\n",
    "        if (tags):\n",
    "            return html_from_file_tags(path)\n",
    "        else:\n",
    "            return html_from_file_no_tags(path)\n",
    "    elif path.endswith('.txt'):\n",
    "        return str(text_from_file(path))\n",
    "    else:\n",
    "        try:\n",
    "            return str(text_from_binary(path))\n",
    "        except:\n",
    "            print(\n",
    "                'Failed to load as binary. Try reader that accepts url as argument (e.g., html_from_web_tags(url) or html_from_web_no_tags(url)).')\n",
    "\n",
    "\n",
    "def text_from_binary(file_path):\n",
    "    #text = textract.process(file_path, method='tesseract', language='eng')\n",
    "    text = 'Not set up for binary'\n",
    "    return text.decode('unicode_escape').encode('utf-8', 'ignore').strip()\n",
    "\n",
    "\n",
    "def html_from_file_tags(file_path):\n",
    "    with open(file_path, 'rb') as myfile:\n",
    "        raw_text = myfile.read()\n",
    "    return raw_text\n",
    "\n",
    "\n",
    "def html_from_web_no_tags(url):\n",
    "    response = urlopen(url)\n",
    "    bsObj = BeautifulSoup(response, 'lxml').text\n",
    "    return bsObj\n",
    "\n",
    "\n",
    "def html_from_web_tags(url):\n",
    "    response = urlopen(url)\n",
    "    tagged_text = response.read()\n",
    "    return tagged_text\n",
    "\n",
    "\n",
    "def text_from_file(file_path):\n",
    "    with open(file_path, 'rb') as myfile:\n",
    "        raw_text = myfile.read()\n",
    "    return raw_text\n",
    "\n",
    "\n",
    "def remove_stop_words(doc):\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    doc = ' '.join(filtered_tokens).lower()\n",
    "    return doc\n",
    "\n",
    "\n",
    "def normalize_document_return_list(doc):\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    list_of_clean_sents = []\n",
    "    sent_list = tokenize.sent_tokenize(str(doc))\n",
    "    for sent in sent_list:\n",
    "        sent = re.sub(r'[^a-zA-Z\\s]', '', sent, re.I | re.A)\n",
    "        sent = sent.strip()\n",
    "        tokens = wpt.tokenize(sent)\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        sent = ' '.join(filtered_tokens).lower()\n",
    "        list_of_clean_sents.append(sent)\n",
    "    return list_of_clean_sents\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    joined_text = \" \".join(stems)\n",
    "    print(joined_text)\n",
    "    return joined_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://s3.amazonaws.com/blaze4/HK+logo.png)\n",
    "# The H&K Lawyer Notebook\n",
    "\n",
    "A collection of simple, yet powerful tools for lawyers to do their jobs better and more efficiently. By leveraging data analytics, natural language processing, machine learning, network graphing and other open source solutions, users are able to produce better quality work faster and more efficiently. Many of these tools are still under active development, so the H&K Lawyer Notebook is reserved for our more intrepid lawyers.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Tool\n",
    "\n",
    "Classify text using one of several trained machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089de443ab73472fb387172000fbc9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Select model:', options=('finance', 'leasing', 'private equity', 'corporate M&A', 'b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00f826b87e84b4c9c8fc28386fc45cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Select:', index=(2,), options=('Termination', 'Commitment/Unused Fees', 'Change in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e3d4ac471b4ff2a91a41381a39f3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='credit.txt', description='Filename:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0953244c748480da0ad784287ed5f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle(button_color='lightblue'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-47edc42640b2>\u001b[0m in \u001b[0;36mon_button_clicked\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mX_train_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "df = pd.read_excel('/Users/josiasdewey/jupyter/CAN/assets/spreadsheets/data_b.xlsx')\n",
    "df = df[pd.notnull(df['Text'])]\n",
    "col = ['Label', 'Text']\n",
    "df = df[col]\n",
    "df.columns = ['Label', 'Text']\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df['category_id'] = df['Label'].factorize()[0]\n",
    "from io import StringIO\n",
    "category_id_df = df[['Label', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'Label']].values)\n",
    "df.head()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(df.Text).toarray()\n",
    "labels = df.category_id\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Label'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "cv_df.groupby('model_name').accuracy.mean()\n",
    "'''\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "model = LinearSVC()\n",
    "#X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)\n",
    "clf = model.fit(X_train, y_train)\n",
    "clf = LinearSVC().fit(X_train_tfidf, y_train)\n",
    "y = [] ###################################\n",
    "for index, row in dfc.iterrows():\n",
    "    y.append(dfc['query'][index])\n",
    "x = clf.predict(count_vect.transform(dfc['query']))\n",
    "for i in (range(len(y))):\n",
    "    print('*********************************************************************************')\n",
    "    print(y[i])\n",
    "    print('')\n",
    "    print(x[i])\n",
    "    print('*********************************************************************************')\n",
    "    \n",
    "    '''\n",
    "\n",
    "\n",
    "'''name = 'projects/ccapp-141701/locations/us-central1/models/TCN1260328743632064609'\n",
    "prediction_client = automl_v1beta1.PredictionServiceClient.from_service_account_json('ccapp-141701-bf3fd9e2a4f7-demo.json')\n",
    "def automl_predict(model_full_id, snippet):\n",
    "    payload = {'text_snippet': {\n",
    "        'content': snippet,\n",
    "        \"mime_type\": \"text/plain\"\n",
    "            }\n",
    "      }\n",
    "    return prediction_client.predict(model_full_id, payload)\n",
    "'''\n",
    "radio_buttons = widgets.RadioButtons(\n",
    "    options=['finance', 'leasing', 'private equity', 'corporate M&A', 'bond financing'],\n",
    "     value='finance',\n",
    "    description='Select model:',\n",
    "    disabled=False\n",
    ")\n",
    "display(radio_buttons)\n",
    "\n",
    "clause_selection = widgets.SelectMultiple(\n",
    "    options=['Termination', 'Commitment/Unused Fees', 'Change in Control', 'Voluntary Prepayment', 'Material Adverse Condition', 'Events of Default', 'Eurodollar Rate'],\n",
    "    value=['Change in Control'],\n",
    "    #rows=10,\n",
    "    description='Select:',\n",
    "    disabled=False\n",
    ")\n",
    "display(clause_selection)\n",
    "\n",
    "file_path_text = widgets.Text(              \n",
    "    value='credit.txt',\n",
    "    placeholder='Type something',\n",
    "    description='Filename:',\n",
    "    disabled=False\n",
    ")\n",
    "display(file_path_text)\n",
    "\n",
    "\n",
    "button = widgets.Button(description=\"Submit\")\n",
    "button.style.button_color = 'lightblue'\n",
    "display(button)\n",
    "\n",
    "download_button = widgets.Button(\n",
    "    description='Download Excel',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click me',\n",
    ")\n",
    "\n",
    "in_progress = widgets.IntProgress(\n",
    "    value=2,\n",
    "    min=0,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Loading:',\n",
    "    bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Label'], random_state = 0)\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(X_train)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "    #model = LinearSVC()\n",
    "    #X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)\n",
    "    #model.fit(X_train, y_train)\n",
    "    from sklearn.externals import joblib\n",
    "    #joblib.dump(model, 'text1.joblib') \n",
    "    current_raw_sent_list = []\n",
    "    current_clean_sent_list = []\n",
    "    sent_list = sent_tokenize(load_raw(file_path_text.value))\n",
    "    in_progress.value = 4\n",
    "    for sent in sent_list:\n",
    "        clean_sent = normalize_sent(sent)\n",
    "        current_clean_sent_list.append(clean_sent)\n",
    "        current_raw_sent_list.append(sent)\n",
    "    df_current = pd.DataFrame(columns=['label', 'raw', 'clean'])\n",
    "    df_current['clean'] = current_clean_sent_list\n",
    "    df_current['raw'] = current_raw_sent_list\n",
    "    model = joblib.load('text1.joblib') \n",
    "    count_vect = CountVectorizer()\n",
    "    cv = count_vect.fit_transform(df_current['clean'])\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tcv = X_train_tfidf = tfidf_transformer.fit_transform(cv)\n",
    "    for index, row in df_current.iterrows():\n",
    "        z = clf.predict(df_current['label'][6])\n",
    "        df_current['label'][index] = z[i]\n",
    "        print('*********************************************************************************')\n",
    "        print(row['clean'])\n",
    "        print('is:')\n",
    "        print(z[i])\n",
    "        print('*********************************************************************************')\n",
    "    display(df_current)\n",
    "    in_progress.value = 10\n",
    "    \n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Compare\n",
    "Use One2Many to compare a large number of documents against a common base document.  For example, compare lease agreements against lease form to determine deviations from approved form. Use Many2Many to compare a large number of items (the 'Query') against another large groups of items (the 'Search') and determine matches. For example, a client requests we confirm the destruction of files relating to several hundred matters, where the matter description for each must be cross checked against several thousand entries in our file storage list. This tool leverages fuzzy matching in order to identify matches undetected by pure booleen searches (e.g., \"FIRST INTERNATIONAL\" captured even if search query is \"INTL FIRS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cbbfe5f50142afb53726a1cdba5873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(VBox(children=(HBox(children=(Text(value='Stock Purchase Agreement_original.txt',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "button_compare = widgets.Button(description=\"Submit\")\n",
    "button_compare.style.button_color = 'lightblue'\n",
    "\n",
    "@button_compare.on_click\n",
    "def on_click(b):\n",
    "    doc_1_raw_sent_list = []\n",
    "    doc_1_clean_sent_list = []\n",
    "    sent_list = sent_tokenize(load_raw(file_path_text_1.value))\n",
    "    for sent in sent_list:\n",
    "        clean_sent = normalize_sent(sent)\n",
    "        doc_1_clean_sent_list.append(clean_sent)\n",
    "        doc_1_raw_sent_list.append(sent)\n",
    "    df_doc_1 = pd.DataFrame(columns=['raw', 'clean'])\n",
    "    df_doc_1['clean'] = doc_1_clean_sent_list\n",
    "    df_doc_1['raw'] = doc_1_raw_sent_list\n",
    "    doc_2_raw_sent_list = []\n",
    "    doc_2_clean_sent_list = []\n",
    "    sent_list = sent_tokenize(load_raw(file_path_text_2.value))\n",
    "    for sent in sent_list:\n",
    "        clean_sent = normalize_sent(sent)\n",
    "        doc_2_clean_sent_list.append(clean_sent)\n",
    "        doc_2_raw_sent_list.append(sent)\n",
    "    df_doc_2 = pd.DataFrame(columns=['raw', 'clean'])\n",
    "    df_doc_2['clean'] = doc_2_clean_sent_list\n",
    "    df_doc_2['raw'] = doc_2_raw_sent_list\n",
    "    df_results = pd.DataFrame(columns=['query', 'match', 'strength'])\n",
    "    query = df_doc_1['clean']\n",
    "    choices = df_doc_2['clean']\n",
    "    query_list = []\n",
    "    found_list = []\n",
    "    strength_list = []\n",
    "    for q in query:\n",
    "        match = process.extractOne(q, choices=choices, scorer=fuzz.token_sort_ratio, score_cutoff=0)\n",
    "        if match != None:\n",
    "            index = match[2]\n",
    "            if match[1] >= compare_slider.value:\n",
    "                found = match[0]\n",
    "                strength = match[1]\n",
    "            else:\n",
    "                found = 'None'\n",
    "                strength = 'N/A'\n",
    "            query_list.append(q)\n",
    "            found_list.append(choices[index])\n",
    "            strength_list.append(strength)\n",
    "    df_results['query'] = query_list\n",
    "    df_results['found']= found_list\n",
    "    df_results['strength'] = strength_list\n",
    "    display(df_results)\n",
    "\n",
    "file_path_text_1 = widgets.Text(\n",
    "    value='Stock Purchase Agreement_original.txt',\n",
    "    placeholder='Type something',\n",
    "    description='Query:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "file_path_text_2 = widgets.Text(\n",
    "    value='Stock Purchase Agreement_new_deal.txt',\n",
    "    placeholder='Type something',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "s = widgets.Text(\n",
    "    value='filename and path',\n",
    "    placeholder='Type something',\n",
    "    description='Base:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "t = widgets.Text(\n",
    "    value='filename and path',\n",
    "    placeholder='Type something',\n",
    "    description='Compare:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "compare_slider = widgets.FloatSlider(\n",
    "    value=0,\n",
    "    base=100,\n",
    "    min=0, # max exponent of base\n",
    "    max=100, # min exponent of base\n",
    "    step=0.2, # exponent step\n",
    "    description='Similarity:'\n",
    ")\n",
    "\n",
    "\n",
    "tab1 = VBox(children=[HBox(children=[file_path_text_1, file_path_text_2, compare_slider])])\n",
    "tab2 = VBox(children=[HBox(children=[s, t])])\n",
    "tab = widgets.Tab(children=[tab1, tab2])\n",
    "tab.set_title(0, 'Many2Many')\n",
    "tab.set_title(1, 'One2Many')\n",
    "VBox(children=[tab, button_compare])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Precedent\n",
    "Quickly search one or more curated clause banks for similar clauses.  Also, the user can identify form documents from which a particular agreememt originates.  The user can set a minimum similarity threshold to filter out less relevant clauses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n",
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    }
   ],
   "source": [
    "button_compare = widgets.Button(description=\"Submit\")\n",
    "button_compare.style.button_color = 'lightblue'\n",
    "\n",
    "@button_compare.on_click\n",
    "def on_click(b):\n",
    "    doc_1_raw_sent_list = []\n",
    "    doc_1_clean_sent_list = []\n",
    "    sent_list = sent_tokenize(load_raw(file_path_text_1.value))\n",
    "    for sent in sent_list:\n",
    "        clean_sent = normalize_sent(sent)\n",
    "        doc_1_clean_sent_list.append(clean_sent)\n",
    "        doc_1_raw_sent_list.append(sent)\n",
    "    df_doc_1 = pd.DataFrame(columns=['raw', 'clean'])\n",
    "    df_doc_1['clean'] = doc_1_clean_sent_list\n",
    "    df_doc_1['raw'] = doc_1_raw_sent_list\n",
    "    doc_2_raw_sent_list = []\n",
    "    doc_2_clean_sent_list = []\n",
    "    sent_list = sent_tokenize(load_raw(file_path_text_2.value))\n",
    "    for sent in sent_list:\n",
    "        clean_sent = normalize_sent(sent)\n",
    "        doc_2_clean_sent_list.append(clean_sent)\n",
    "        doc_2_raw_sent_list.append(sent)\n",
    "    df_doc_2 = pd.DataFrame(columns=['raw', 'clean'])\n",
    "    df_doc_2['clean'] = doc_2_clean_sent_list\n",
    "    df_doc_2['raw'] = doc_2_raw_sent_list\n",
    "    df_results = pd.DataFrame(columns=['query', 'match', 'strength'])\n",
    "    query = df_doc_1['clean']\n",
    "    choices = df_doc_2['clean']\n",
    "    query_list = []\n",
    "    found_list = []\n",
    "    strength_list = []\n",
    "    for q in query:\n",
    "        match = process.extractOne(q, choices=choices, scorer=fuzz.token_sort_ratio, score_cutoff=0)\n",
    "        if match != None:\n",
    "            index = match[2]\n",
    "            if match[1] >= compare_slider.value:\n",
    "                found = match[0]\n",
    "                strength = match[1]\n",
    "            else:\n",
    "                found = 'None'\n",
    "                strength = 'N/A'\n",
    "            query_list.append(q)\n",
    "            found_list.append(choices[index])\n",
    "            strength_list.append(strength)\n",
    "    df_results['query'] = query_list\n",
    "    df_results['found']= found_list\n",
    "    df_results['strength'] = strength_list\n",
    "    display(df_results)\n",
    "\n",
    "file_path_text_1 = widgets.Text(\n",
    "    value='Stock Purchase Agreement_original.txt',\n",
    "    placeholder='Type something',\n",
    "    description='Query:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "file_path_text_2 = widgets.Text(\n",
    "    value='Stock Purchase Agreement_new_deal.txt',\n",
    "    placeholder='Type something',\n",
    "    description='Search:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "number_returned = widgets.Dropdown(\n",
    "    options=['1', '2', '3'],\n",
    "    value='2',\n",
    "    description='Number:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "t = widgets.Text(\n",
    "    value='filename and path',\n",
    "    placeholder='Type something',\n",
    "    description='Compare:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "compare_slider = widgets.FloatSlider(\n",
    "    value=0,\n",
    "    base=100,\n",
    "    min=0, # max exponent of base\n",
    "    max=100, # min exponent of base\n",
    "    step=0.2, # exponent step\n",
    "    description='Similarity:'\n",
    ")\n",
    "\n",
    "\n",
    "tab1 = VBox(children=[HBox(children=[file_path_text_1, file_path_text_2, compare_slider])])\n",
    "tab2 = VBox(children=[HBox(children=[number_returned, t])])\n",
    "tab = widgets.Tab(children=[tab1, tab2])\n",
    "tab.set_title(0, 'Many2Many')\n",
    "tab.set_title(1, 'One2Many')\n",
    "VBox(children=[tab, button_compare])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and Text Similarity Models\n",
    "*Cluster text into similar topics.  Find similar text within several documents.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (UnrecognizedClientException) when calling the StartTopicsDetectionJob operation: The security token included in the request is invalid.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-49a6ebfc6638>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                                               \u001b[0mInputDataConfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                                                               \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                                                                               DataAccessRoleArn=data_access_role_arn)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start_topics_detection_job_result: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_topics_detection_job_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    319\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (UnrecognizedClientException) when calling the StartTopicsDetectionJob operation: The security token included in the request is invalid."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import bson\n",
    "#from bson import json_util\n",
    " \n",
    "comprehend = boto3.client(service_name='comprehend')\n",
    "                \n",
    "input_s3_url = \"s3://amazonaws.com/topichk/xyz.txt\"\n",
    "input_doc_format = \"ONE_DOC_PER_LINE\"\n",
    "output_s3_url = \"s3://amazonaws.com/blaze4/output.json\"\n",
    "data_access_role_arn = 'arn:aws:iam::701155529821:role/service-role/AmazonComprehendServiceRole-comprehend'\n",
    "number_of_topics = 10\n",
    " \n",
    "input_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\n",
    "output_data_config = {\"S3Uri\": output_s3_url}\n",
    " \n",
    "start_topics_detection_job_result = comprehend.start_topics_detection_job(NumberOfTopics=number_of_topics,\n",
    "                                                                              InputDataConfig=input_data_config,\n",
    "                                                                              OutputDataConfig=output_data_config,\n",
    "                                                                              DataAccessRoleArn=data_access_role_arn)\n",
    " \n",
    "print('start_topics_detection_job_result: ' + json.dumps(start_topics_detection_job_result))\n",
    " \n",
    "job_id = start_topics_detection_job_result[\"JobId\"]\n",
    " \n",
    "print('job_id: ' + job_id)\n",
    " \n",
    "describe_topics_detection_job_result = comprehend.describe_topics_detection_job(JobId=job_id)\n",
    " \n",
    "print('describe_topics_detection_job_result: ' + json.dumps(describe_topics_detection_job_result, default=json_util.default))\n",
    " \n",
    "list_topics_detection_jobs_result = comprehend.list_topics_detection_jobs()\n",
    " \n",
    "print('list_topics_detection_jobs_result: ' + json.dumps(list_topics_detection_jobs_result, default=json_util.default))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuickCAN\n",
    "Computer generated revisions to contracts based on precedent clause database.  Machine learning algorithms and fuzzy matching are used to incorporate revisions made to similar text in precedent clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RadioButtons(description='Type:', options=('finance', 'leasing', 'private equity', 'corporate M&A', 'bond fina…"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "SelectionSlider(continuous_update=False, description='Set Intensity:', options=('Aggressive', 'Favorable', 'Ne…"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(value='filename and path', description='Filename:', placeholder='Type something')"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Button(description='Submit', style=ButtonStyle(button_color='lightblue'))"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = widgets.RadioButtons(\n",
    "    options=['finance', 'leasing', 'private equity', 'corporate M&A', 'bond financing'],\n",
    "     value='finance',\n",
    "    description='Type:',\n",
    "    disabled=False\n",
    ")\n",
    "display(x)\n",
    "\n",
    "IntSlider = widgets.SelectionSlider(\n",
    "    options=['Aggressive', 'Favorable', 'Neutral', 'Must Haves'],\n",
    "    value='Aggressive',\n",
    "    description='Set Intensity:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True\n",
    ")\n",
    "\n",
    "display(IntSlider)\n",
    "\n",
    "y = widgets.Text(\n",
    "    value='filename and path',\n",
    "    placeholder='Type something',\n",
    "    description='Filename:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "display(y)\n",
    "\n",
    "from IPython.display import display\n",
    "button = widgets.Button(description=\"Submit\")\n",
    "button.style.button_color = 'lightblue'\n",
    "display(button)\n",
    "\n",
    "def on_button_clicked(df_current, df_precedent):\n",
    "    print('')\n",
    "    #run_match(df_current, df_precedent)\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked(y.value, y.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QuickCAR\n",
    "\n",
    "Computer generated responses to requests for production and interrogetories. Machine learning algorithms and fuzzy matching are used to incorporate historical responses to similar requests.  Will automatically generate a first draft of a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signature Page Generator\n",
    "***\n",
    "Generate signature page packets with the press of a button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Binder Generator\n",
    "***\n",
    "Automate the preparation of closing binders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(Tab(children=(VBox(children=(HBox(children=(RadioButtons(description='Lender:', index=4, option…"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@button.on_click\n",
    "def on_click(b):\n",
    "    pass\n",
    "\n",
    "lender_widget = widgets.RadioButtons(\n",
    "    options=['Wells Fargo', 'Bank of America (Private Wealth)', 'Bank of America', 'JPMorgan Chase', 'Ocean Bank', 'Other'],\n",
    "    value='Ocean Bank',\n",
    "    description='Lender:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "toggle = widgets.RadioButtons(\n",
    "    options=['Middle Market', 'Construction Loan', 'Bank Loan', 'Loan Modification', 'ABL Loan', 'Art Loan'],\n",
    "    value='Bank Loan',\n",
    "    description='Loan Type:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "title_textbox = widgets.Text(\n",
    "    value='RM',\n",
    "    description='Title:',\n",
    ")\n",
    "\n",
    "tab1 = VBox(children=[HBox(children=[lender_widget, toggle])])\n",
    "tab2 = VBox(children=[HBox(children=[title_textbox,])])\n",
    "tab = widgets.Tab(children=[tab1, tab2])\n",
    "tab.set_title(0, 'Loan Details')\n",
    "tab.set_title(1, 'Contacts')\n",
    "VBox(children=[tab, button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('/Users/josiasdewey/jupyter/CAN/assets/spreadsheets/data_b.xlsx')\n",
    "df = df[pd.notnull(df['Text'])]\n",
    "col = ['Label', 'Text']\n",
    "df = df[col]\n",
    "df.columns = ['Label', 'Text']\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df['category_id'] = df['Label'].factorize()[0]\n",
    "from io import StringIO\n",
    "category_id_df = df[['Label', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'Label']].values)\n",
    "df.head()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(df.Text).toarray()\n",
    "labels = df.category_id\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Label'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LogisticRegression instance is not fitted yet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-1fc578836ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_current\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdf_current\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coef_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             raise NotFittedError(\"This %(name)s instance is not fitted \"\n\u001b[0;32m--> 298\u001b[0;31m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet"
     ]
    }
   ],
   "source": [
    "current_raw_sent_list = []\n",
    "current_clean_sent_list = []\n",
    "sent_list = sent_tokenize(load_raw(file_path_text.value))\n",
    "in_progress.value = 4\n",
    "for sent in sent_list:\n",
    "    clean_sent = normalize_sent(sent)\n",
    "    current_clean_sent_list.append(clean_sent)\n",
    "    current_raw_sent_list.append(sent)\n",
    "df_current = pd.DataFrame(columns=['label', 'raw', 'clean'])\n",
    "df_current['clean'] = current_clean_sent_list\n",
    "df_current['raw'] = current_raw_sent_listcv = count_vect.fit_transform(df_current['clean'])\n",
    "cv = count_vect.fit_transform(df_current['clean'])\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "z = tfidf_transformer.fit_transform(cv)\n",
    "for index, row in df_current.iterrows():\n",
    "    z = model.predict(cv)\n",
    "    df_current['label'][index] = x[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "217.717px",
    "left": "38px",
    "top": "123.867px",
    "width": "261.283px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
